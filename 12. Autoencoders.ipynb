{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12ec631-0d57-4a6d-a0ec-7940abac6968",
   "metadata": {},
   "source": [
    "## 12. Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9021294f-5f48-4f53-8b36-773852531a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.396090</td>\n",
       "      <td>2.092611</td>\n",
       "      <td>2.073392</td>\n",
       "      <td>1.988262</td>\n",
       "      <td>1.953473</td>\n",
       "      <td>2.450997</td>\n",
       "      <td>1.631040</td>\n",
       "      <td>1.746182</td>\n",
       "      <td>1.898050</td>\n",
       "      <td>2.380148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703454</td>\n",
       "      <td>2.502966</td>\n",
       "      <td>2.119108</td>\n",
       "      <td>2.106098</td>\n",
       "      <td>2.165173</td>\n",
       "      <td>2.340826</td>\n",
       "      <td>2.170109</td>\n",
       "      <td>1.749139</td>\n",
       "      <td>1.678661</td>\n",
       "      <td>1.829647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.775596</td>\n",
       "      <td>1.829438</td>\n",
       "      <td>2.054768</td>\n",
       "      <td>1.577190</td>\n",
       "      <td>1.594549</td>\n",
       "      <td>1.373357</td>\n",
       "      <td>1.946647</td>\n",
       "      <td>1.841420</td>\n",
       "      <td>1.595761</td>\n",
       "      <td>2.538094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974274</td>\n",
       "      <td>1.621608</td>\n",
       "      <td>2.003085</td>\n",
       "      <td>2.076871</td>\n",
       "      <td>1.788868</td>\n",
       "      <td>2.062829</td>\n",
       "      <td>2.084499</td>\n",
       "      <td>2.267568</td>\n",
       "      <td>1.536939</td>\n",
       "      <td>2.132725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.835679</td>\n",
       "      <td>1.612100</td>\n",
       "      <td>2.174908</td>\n",
       "      <td>2.084460</td>\n",
       "      <td>2.472896</td>\n",
       "      <td>2.029110</td>\n",
       "      <td>2.410107</td>\n",
       "      <td>2.282164</td>\n",
       "      <td>2.208201</td>\n",
       "      <td>2.106240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.035652</td>\n",
       "      <td>2.065291</td>\n",
       "      <td>2.197711</td>\n",
       "      <td>2.288806</td>\n",
       "      <td>2.480274</td>\n",
       "      <td>1.946207</td>\n",
       "      <td>1.947120</td>\n",
       "      <td>1.754344</td>\n",
       "      <td>2.265033</td>\n",
       "      <td>2.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.420241</td>\n",
       "      <td>2.158485</td>\n",
       "      <td>1.958602</td>\n",
       "      <td>1.903787</td>\n",
       "      <td>2.230522</td>\n",
       "      <td>1.984789</td>\n",
       "      <td>1.964441</td>\n",
       "      <td>2.360795</td>\n",
       "      <td>1.820773</td>\n",
       "      <td>2.116560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.040977</td>\n",
       "      <td>1.511381</td>\n",
       "      <td>1.834332</td>\n",
       "      <td>2.070046</td>\n",
       "      <td>1.911699</td>\n",
       "      <td>1.816916</td>\n",
       "      <td>2.213950</td>\n",
       "      <td>2.099758</td>\n",
       "      <td>2.259999</td>\n",
       "      <td>2.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.646926</td>\n",
       "      <td>1.778067</td>\n",
       "      <td>1.760959</td>\n",
       "      <td>1.894087</td>\n",
       "      <td>1.888225</td>\n",
       "      <td>2.228021</td>\n",
       "      <td>2.489542</td>\n",
       "      <td>2.326377</td>\n",
       "      <td>1.969615</td>\n",
       "      <td>2.001316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063858</td>\n",
       "      <td>2.341009</td>\n",
       "      <td>1.844115</td>\n",
       "      <td>2.076399</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.969530</td>\n",
       "      <td>1.821128</td>\n",
       "      <td>1.946249</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>1.797722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.396090  2.092611  2.073392  1.988262  1.953473  2.450997  1.631040   \n",
       "1  1.775596  1.829438  2.054768  1.577190  1.594549  1.373357  1.946647   \n",
       "2  1.835679  1.612100  2.174908  2.084460  2.472896  2.029110  2.410107   \n",
       "3  2.420241  2.158485  1.958602  1.903787  2.230522  1.984789  1.964441   \n",
       "4  1.646926  1.778067  1.760959  1.894087  1.888225  2.228021  2.489542   \n",
       "\n",
       "         7         8         9   ...        15        16        17        18  \\\n",
       "0  1.746182  1.898050  2.380148  ...  1.703454  2.502966  2.119108  2.106098   \n",
       "1  1.841420  1.595761  2.538094  ...  1.974274  1.621608  2.003085  2.076871   \n",
       "2  2.282164  2.208201  2.106240  ...  2.035652  2.065291  2.197711  2.288806   \n",
       "3  2.360795  1.820773  2.116560  ...  2.040977  1.511381  1.834332  2.070046   \n",
       "4  2.326377  1.969615  2.001316  ...  2.063858  2.341009  1.844115  2.076399   \n",
       "\n",
       "         19        20        21        22        23        24  \n",
       "0  2.165173  2.340826  2.170109  1.749139  1.678661  1.829647  \n",
       "1  1.788868  2.062829  2.084499  2.267568  1.536939  2.132725  \n",
       "2  2.480274  1.946207  1.947120  1.754344  2.265033  2.119050  \n",
       "3  1.911699  1.816916  2.213950  2.099758  2.259999  2.039066  \n",
       "4  1.742857  1.969530  1.821128  1.946249  1.678283  1.797722  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.utils.data import generate_data\n",
    "contamination = 0.05 # percentage of outliers\n",
    "n_train = 500       # number of training points\n",
    "n_test = 500        # number of testing points\n",
    "n_features = 25      # number of features\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, \n",
    "    n_test=n_test, \n",
    "    n_features= n_features, \n",
    "    contamination=contamination, \n",
    "    random_state=123)\n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train)\n",
    "X_train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bcc7a0-bafc-425e-af0a-e48a584cbf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD5CAYAAAADQw/9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAxOAAAMTgF/d4wjAAA2xklEQVR4nO3deXhU5dn48e9zZk1msoMCBkTZrIoiSt0QULQo9lXrWqtvwRWt6+uurb+qrdr2xbRaa12qttWW17XWFlsVRAVF6oqCiIAsCYpAyJ7Mcs65f3+cMBIIi5jkzCT357rmEs45M+eeSO555lnux4iIoJRSKutZfgeglFJqx2jCVkqpHKEJWymlcoQmbKWUyhGasJVSKkdowlZKqRyhCVsppXKEJmylfPTqq69ijMG2bb9DUTlAE7bKGsuXL+fMM8+kX79+xONx+vXrx8SJE/niiy865PUnT57M2Wef3eZYriXMXItXdSxN2CprTJw4kYKCAhYsWEBjYyPvv/8+Z5xxBsYYv0PbrlQq5XcIqicQpbLA+vXrBZB33313m9e9+eabcuSRR0pZWZmUlJTIuHHjpLm5WUREbr75ZhkyZIjE43EpLy+XSy+9VJqamkRE5Pbbb5dgMCjBYFBisZjEYjFZuXKlRKNRATLHbr/9dhERqampkYsuukgGDBggpaWlctxxx8myZcsycUyaNElOO+00ueiii6RXr15y7LHHthvv2LFj5ZJLLpHvfe97Eo/HZdCgQfKnP/0pc37WrFkCSDqdFhER27blV7/6lQwZMkQKCwvlwAMPlBdeeEFEZJvxqp5BE7bKGsOHD5eDDjpIHnnkEZk/f744jtPm/IIFCyQajcq9994rTU1NkkwmZdasWZJIJERE5M9//rOsXLlSXNeVBQsWyKBBg+SGG27IPH/SpEly1llntXnNzROmiIjrujJu3Dj5wQ9+INXV1ZJIJOS6666Tb33rW5JKpTKvFQwG5eGHH5ZUKpX5YNjc2LFjJRqNyvPPPy/pdFqmT58uoVBI5syZ0+79p06dKrvttpu8++67kk6nZdq0aRIKhTIfZO3Fq3oOTdgqa6xfv15uvvlmGTVqlEQiESkpKZGrr746k5AvueQSOf7443f49SoqKmTkyJGZv+9own733XclFApJQ0ND5pht2xKNRmX27NmZ1zrkkEO2G8PYsWPl5JNPbnPs9NNPl3PPPbfd+w8dOlR+85vftLn+hBNOkClTpmw1XtVzBP3qilFqc2VlZdx2223cdtttJJNJ/vWvfzFp0iTi8Ti33HILy5cvZ6+99trq8x944AEeeOABVq5ciW3bpNNpysrKvnYcS5YswbZtysvLtzhXWVmZ+fMee+yxQ6+3+XV77LEH7733XrvXVlZWMmjQoDbHBg8ezKJFi3boXqp704StslIkEuGkk07i6KOPziS3gQMH8umnn7Z7/dy5c7n00kt56aWXGD16NKFQiF//+tfcddddmWssa8sx9vaO9enTh3A4zLp16wiFQluNsb3ntmfFihVb/L29DwOA/v37s2zZsjbHli1bxoABA77WPVX3pP/3VVaoqanhhhtu4MMPPySZTOI4DjNnzmTWrFmMGTMGgIsvvpiXX36Z+++/n5aWFtLpNK+99hrJZJK6ujoCgQC9e/cmFArx3nvvce+997a5R58+fVi2bBmO47Q5BrB48eLMsdGjR7Pvvvty8cUXs3bt2kx8zzzzDM3NzV/7vb3wwgtMnz4dx3H497//zd/+9jfOOeecdq89//zzmTp1Kh988AG2bfPkk0/ywgsvcP755281XtWD+N0no5SISGNjo5x33nkydOhQicfjUlRUJPvss4/84he/ENd1M9fNnj1bxowZI8XFxVJSUiJHHXWUNDc3i+M4csUVV0hZWZkUFhbKhAkT5NZbb5Xddtst89zly5fLIYccIsXFxVJUVCQrV64UEZHLLrtMevfuLUVFRXLnnXeKiMiGDRvksssuk4EDB0o8Hpf+/fvLWWedlZmR0l5/eHs2nyWy5557yiOPPJI5394skTvvvFMGDRokBQUFMnLkSPnHP/7R5jXbi1f1DEZEd5xRqrOMGzeO0aNH8/Of/9zvUFQ3oF0iSimVIzRhK6VUjtAuEaWUyhHawlZKqRyhCVsppXJETi+ciUQi9O7d2+8wlFKqw6xbt45kMtnuuZxO2L1796aqqsrvMJRSqsNsbRUsaJeIUkrlDE3YSimVI3K6S0Qplb3stM38VxeyZvlaSvoUM/Lo/YjmR/wOK6dpwlZKdbi1q9bxix/+lrWr1oOAMRArjnHdHy9lz/129zu8nKVdIkqpDiUi/PbSh1m7ch0FpXEKexUQL43TXN/CXefdRyqZ9jvEnKUJWynVoVZ9sprlH60iXhrPbKBsjCFWnE/9hkY+fO1jnyPMXZqwlVIdqm5dPYGgtcVmC8YYMN75neW6Lus/30BDTeM3DbONlqYEDTWNZHulDu3DVkp1qH6DdsVxXBzbIRAMZI6LK4gr7Dakz0697pvPv80Tv/o76yrXY4xh2KhBnHDJsex7+F4EQzuWypItSRo2NFJYVkA4GmbtqnU8/rNneG/mh4gjDNinnB/ceDLDj/jWTsXY2XK6+FN5ebkunFEqC/3uykd487m3iRXnEwwFcWyHxpomBo0YyC3PXrvdrc4cx+GVv87h47mLKe1bQlFZIU/86jmCkSCBQID1q6tJNqewAha7713O2TefyujvHbzV10un0jw19XlmPD6bRGOCvIIoR5xyCP/51/vUrasnryAPEZdUcxpjGW547DL2PnRYR/9Ydsi28pq2sJVSHe68O84iGAoy59l5GAOuK4w4cl+mTP0hlmWxdtU6ln6wgmh+hH0OH0Ykz5vuZ6dt3vj7f/jtJQ/TsOGrbg8RIb8wn97lpXy+dA2uKwTDARzbpWZNLfdf/SfyC/IYefR+7cbz4HWP8eZzb5NXEKV41yKSLSme/92LpNNpwLCushpBsCyLaEGEp+76Bz992p+EvS3awlZKdQjHdmiqbya/IC/TRVGzto61q9ZT2qeY3uVlOLbDoz+ZxqtPvIkVsLxEXJDHj35zDnba5qEbHmf5h6twHReAUDiIsQyphNfyLSiJ09zQQiDotdDttEMkL0w4GmL3vfsz9ZVbMvFULl7NK9PeYNkHy/nwtY8p7VtMNBbNnP986Rqa6jbZo9MArdkwGo/y8MJf07u8LDNw2lW2ldc0YSulvhHHdnj+9y/y74dfoaGmkXhxjGPPPZITfnTsFn3Lz/32BZ666x/EimMEQwFEhJaGFuyUg2M7pFM2devrW+dum0yrd2MCt4IGywoQCFo4toOdcrACxrtGhJOvOJ7z7zyLd16az++ueATXdkklU9Stb8CyLArLCojkhcmLR/l82RoSTV6RJWMZb8Bxk2zYa7dSdt+nnB/ecgZ7HzK0y36emrCVUp3m4Zv+wit/nUMkFiESDZNMpEg2JRh3xmgu+OXZmescx+FHB11POmkTjoZIJdJYAUM6abPmsy9xXfESp9uakjZp8W7KsiwCYYt0wgYgHA0heItz4sUxynYr5bMPVoAxFPcuJJIXZs2KdV7SN2Q+RFzHzXwQtKd8SF/SKa/L5P89fQ1DD9yzY35g26EJWynVKdZVVfM/Y24mvzCvTWvaTjs01zdT8eqt7DLAK4HcVNfEBcOvxhWhfn09ruPiOG7bpLyVJL2F1uuCoQACuLZLIBTATtlbXBoMB7BTTuZ5oXAIYyCV2PYCnkh+mFRLGhHBCliM/t63uf6xywlHQjsQ4M7bVl7TedhKqZ322YcrsQLWFl0fwVAAK2CxbP7KzLFoPIqIS/XnG0gnbRzb3TI5bydZx1u7UjbO6bbTDk7aQUTaTdYAdsrBWCaT5F3HwXW9JLwtyeYU0hqQ67i8/sxbXDzyWhzH2XaQnUgTtlJqp+XFo9786s2+qIt4c67z4t4gn+u6fDJvCTVr63esBb0VjbVN2GnH6zb5Gq8jrhAMBTEBQ7wkTmmfYnbdYxeswHYGFKXtn1ctWs3PT6/wLWlrwlZK7bCm+ma+WP4liWZvsO5bhwwhXhKjub6lzXXN9S3kF+YRL43z/isfcdmhN3HdMT/DSfvXOrVTNuIKqUSKQNDCtR16lfciXhwjFAlhBSyMAdNeEjfewCTA+68s4P2ZC7o4eo/Ow1ZKbVdzQwt/+fnTzHl2HnbaIRqL8p3J4zjlyuO59J7zuOu8+6hb38DGGXB22iEYDPDTE3/JuqrqrwYS/SbQ0pCgpSFBMBRgtyF9WF9VjWUZ8gqipBM2qURqq88FL3HPff5tDvrO/l0XdytN2EqpbRIR7r74QRbMWUR+UYxYONi68OTfJJoSTLrlDH4186fMeXYeq5d8gTGGOX+bh+O4JJoT2ZOsN2OnHaqWrCGSHybRlKSptnVO9uYDn6Z1iqErGGPIK8jLfMPoatolopTapmUfrGDhG4spKCsgFPbaeJG8MPlF+cx8fDa16+ro1a+Uky49jkvuPpfPl62hpSlBc0MLjRuafI5+25y0Q0vjZh8qApZl2vzdS9ZQ0rcYYwwHHtP1rWvQFrZSajtWLKzEWGaL+h+hcJBkU5LKTz4n1ZJi8Tuf8cl/lvD2vz/AdduZAZKt2okzFA3h2G5m5kk4P0xx7yLEdSkf0odDTzioi4P0aMJWSm1TrCi/3ePiCo7t8PJjr/Leyx/hilC9esM2F6PkChEhELQQN4CxDNG8CLHCPI445WBOvPQ48jZZ4t6VNGErpbZpxFH7Es2P0FTfTKzwq+TdUNNIJD/Cuy99SKw4RmNNY7dI1gDppA0iiIAVtNjr24O57fnrCQQC239yJ9I+bKXUNuXFolx+3wUELIv66gZq19ZRX91AQUmcYDhAOC9MMBQg6dNAXGfw5paDFTAYY1j89lKe+OVzfoelS9OVUtvnui5L3v2M92ctIJ1I039YPw44ej8uGXU9saJ8Uok0X65cR3o7y71zkbEMJX2KCUdD3PvWnRT1Ktzqtas+Wc3KhZXEimPsO3qvnVrGrvWwlVI7beGbi3n05ml8sexLAPoM3IX9x+1DYWmcsn6lrFpURaIxge3jopjOJK5Qu7aOaCzCioWV7D92ny2uSTQn+d0Vj/D+jI8yZWPjxTGu+P0F7PXtIR0Wi3aJKKW2auWiKv73nN+xduV6CssKKOxVwLrV1Uw97z4++3Al+x4+jOb6FtwsnWvdUVzbpbmuhfde/rDd83+9/Rnee/lD4iWxzKOpvpmp595H/YaGDotDE7baYeKsQdILEKfa71BUF3nhoRnYKZt4SQxjef258eIYru3yj/tf4ovlazEB020GG7fnhT/MYMXCyjbHmhtaeP3pueQX5mUKSm38OSWak8z753sddv+sSNiJRIKTTjqJoUOHsv/++3PMMcewdOlSv8NSrcStxa29Ean+AVJ7BbLhNNz6OxFp2f6TVU5b8u5nhPO27IeN5IdZ/J8lvD/zI2/RSdduyuILYxlcV5jx2GttjtetqyedtAm19ld79UrSOLaD67isrVzfYTFkRcIGuPDCC1m8eDHz58/nxBNP5Pzzz/c7JEVr1bW6myA1F0wBmCIgBokXkYapfofX6UQSSHoxYq/aoiJdT1DUu5B0O2VL7bSDnXa8YknG7PCu5blMXCGSF6Zy8edtjhfvWuRtyNCSomZNLSsXVbF6yResWrSaxpomintvfZDy68qKhB2NRpk4cWJm77RDDjmEFStW+BuU8tgfQXoRmGIwrXNQTQhMISReQZw1vobXWUQEt/lpZP0pSM3FyIYfIrVTEHuZ36F1qWP+eyyu7eLYXw0oOraDnbbJL8xDpPVn5WON6C5jvCqEfffcpc3hvFiUo/97LOtXb6BmbR0GbzqgiJBOppn97Dxv5WcHyIqEvbm7776bE0880e8wFIC9EozlPTZlgl4Ct1e2/7wcJ4kXoPFewPW+VZhisJcgtVchbq3P0XUuEWHhm4u5738eZdYTc9htSF8aqhupXVtH7do6mmqb2fuQoaxatJrmupZMrY1uT6ClsYU1K9Yx+9l5vPXPd2mq82qlHH/h0ZlKhd4HmBAKh+g7uA9Viz9nwZxPOiSErPsec8cdd7B06VJmzpy5xbmKigoqKioyf29sbOzK0HomqxTEpXVX1K+Oiws4ECjzK7JOI+JC85+BMJi81qMGKAG3Fkm8hMk/3ccIO57ruqyvqiYQCjJr2hz+fu+/cEUIBAM4rTuTjzn9UIp7F7HPYcO455KHcB3psJZjrojG85j7/Dt88p+lhCJBAoEAk392BrsN7ktBaZxe/Xt59bYDFpH8CMYYkk1JVi1azX5j9v7G98+qhD116lSeffZZZsyYQX7+lvULrrrqKq666qrM38vLy7syvJ4pPMpL2m4NUOQlbRGQOggNgcAgvyPseNIEzhowJe2fTy/q2ng62fuvfMTjtz3NmhVrsdM2TbXN9CovpWCTZej16+tZtWg15/zsTBbNW8LaVdUkmhNYASu3Cj19A8FwkFRLqrXfHgpK4iSbkzx841+55J5zcR0hELDaLN8HsAIWBSWxDokha7pEKioqmDZtGi+//DLFxcV+h6NaGRPGFN0OVqGXpN0a77+BfpjCWzLjDt2KibS2rNtbtedCYJd2juemRfOWUHHB/axbXU1hrwKMZUinbNZVVrfZIzFeEufTd5axYU0NiaYEiaYElmURDAcIRYLd89/BZpxNBlk3iuRHEFd4f+ZHDB01iIYNjW0Gp5sbWgiFgxw0oWPKsWZFC7uqqoqrr76aPffckyOPPBKASCTCvHnzfI5MAZjQXlA2DZJvgrsOAuUQPhhjOnf3aL8YE0byvgvNTwGhr/rvJQHGwkS/42t8Hem5e15ARCgojmeObZy+Vl/dQGnfkswx8HYa33O/3bFtB8Grt+HYLpivt8diLhIR3NZNDPIL8jLHA+EAX3z2JZfecy53nn0P66s2YNsOgaBFKBLiit9fSKyoY1rYWZGwy8vLe+SUqVxiTB5Ex/sdRpcx+ed6M0JSH3j99cYCLIhfhQl2n26gJe8vz2yUCxCNRTGmARASTV8Vc2puaKFXeRm7DOiFZVnsMXwAn769jB4wN6QNx3Yo7l1IJD+SOWanbPoP241dBvTmly/dzLsvf8jqJV9Q1KuQg48fSWFZQYfdPysStlLZxlj5UHQXpOeD/bHXRRI+HBPY1e/QOlS8OJ+GmqbMoo+8eJRoLJLZVNdO25kdWX5w08mZTQzGnnYoS979rFvPDjEGjGVlamNbgQDpZJqCUi8BiwiJxgTBYIDvTBoLQDga5tD/6rzNDTRhK7UVxlgQPsB7dFNHnz2G//vlc0Tyw1iWhTGGst1KQTZQUBqjpSHBwH0HcMqVxzPiyH0zz/tk3hKKdymibl0driNb7oOY4wIhi97lvVr3cIwSCAZIp2w2fFGDY9s01tiICHkFUS74xdnsvnf/LolLE7ZSPdix543nk7eX8uFrH7fWAzFYluF7V0zk7JtPBdhiazCAuvUNxIrySTQmSKdsr84IkE7ZOd3qDoYDuI7gpF1q19YRDHmt6nhJjKa6Fsaccgh9B+3Ky4+9TjqRYv+x+9B3UJ8ui0/rYSvVw7muy8dzP2XB7EUEw0FGHr0fewwfsM2ZHw9d/zivPvkG6aRNw4ZGLMuQTto5PxYVDAdwbDfzoRMIWgjezJDyIX0JhAJUffoFxkBePI9gOEh+QR43P3kVA/fpmFb2tvKaJuxuRCQFqfdBGiE4BBMc4HdIqpuqWvIFN5/wCxJNCWrX1reZApjLrKCFa7uYgMFgyC/0ZoMY463mbG5ItO5Cs7FvO0BBSYwRR+3L9X+6rENi2FZey5p52OqbkdR8pPoMpO5GpOEOZMMPcetv85K4Uh2sfEhfbnjscvYYvjv5Bf5sSNsZXNvFGAiGglgBi10G9KLvnrsSK4rRWNuMsQyBYAArYAgELZy0TTqZ5qPZi0glO3+3HU3Y3YC4G5C6G8FtaK170fpIvII0/cHv8FQ3NeygQdz+zxuZcM5RWJYhFAkSzgtnakJn5NCamlAkhAlYiOOSX5hHIOgVPGuqa/Yu2Kw/wgpYNNW3YIyhK9YOacLuBiQxE2jxViNu/FdjAmBi0PI8Iglf41PdlzGGBW98ghUMAF7SCoaDbZK2ZVkEglZWr4a0AhZWwCKSFwbxEndZv9LMeTttEwhZGMts0U/vOi4HjB9OKNz5C8l0lkh34FR59T02/30wEXDXg1sLga4byVa5L51K89qTc5k1bQ4tjQmGj/kWx503nj4D2y7LFxFq1tRSsmsRNV96U/yMZXktU4GCsjg/+PHJfPjax7zz4nzSyZQ3DTCLBMNBinoVMHCf/uw3bh/en/kRq5d8QTqZzsy1DkfDhKIhxHFpaUwCgjHeKs9YcT5n3vC9LolVBx27Abf5aWi8D6zitickAbiYXs9hTPfpZ1Sdy7Ed7rrg98x/dSHBUJBA0CLVkiYaj3Lzk1ex+7faFl277uhb+XKVt6tK7do6Uok0gYBFOBriv350LOf87PsA1Kyt5b4r/sjsZ97CcRyve8GH+dvGGARvwLC0TzH7jt6L8WeNYdSxI7Asi1QyzQsPzWDG46/TUN3IoBEDmXjB0Tx793SWf7QS1xFaGluwUw6xonz+31NXMfyIb16JbyOdJdLNiVuDVJ8FkvQ2FjAGxAaph/wzseJT/A5R5ZB3XprPb6Y8QLw01mYOdt36evYbs/cWsyFmPzuPB67+E9F4hHA07M2maPTqZN8+/SbKh/Rtc/3dP3qQ2U+/RSgaoubLOpwu3G09FAly0IQRnHjpcew/du+vtVNOU30zLzw0g9nPvEUqkWbEkftywo8m0K+D52FvK69pl0g3YKwSKP4VUn8rONWABeJA9FhM7By/w1M55p0XP0BEtlgwEyuK8dHri0g0J4luUktj9Pe+zYYvavjbPS+Qam5EBOIlMaZM/eEWyRrgrJ+cyuJ3lrHio1WIK16J1i7YxLegNMZ+Y/fmlmeu26nnxwrzOe3qEzjt6hM6OLIdpwm7mzChfaF0mlf7QhogOBQT6Od3WCoHbe1L99bGDI0xnHjJsRx99hEs/WAFoXCQIQfuudVBuF79Shk1YQRL31ueuZcV8OY1b7pK0pgtB/iMZbAsq82WZTsiGA7Q3JBgzKmHfa3nZRudJdKNGBPEhA/ERMZpslY77aDv7I8xZovdZJpqm9nnsGFtWtebihXF2H/sPux96LBtzpiY98J7vPjoLCzLm5URjoYys0iswFefCpsm60AowEmXHcdv37qDIQfukSn3ujkrYLUZfDeWIRAKIOIVZooXb7kxSi7RFrZSqo0Dxg9nn8OHsWDOJwRbt8FKNieJxqKcedPJ3/j1//H7FwlFQwRCAVxXsCyDad20NhyNkE7ZBIIWiaYkxhiisQj5hfnMff4dwtFQppLgxs2PjGXIi+cRK8ojnUp7ibkoRlN9Cy2N3hzpeFE+ruuSTub2ikxN2EqpNoKhIFc//CNe+ctsZv3fHFoaEhz6Xwdx/IVHd8gA2+qla4jmRyjtU8z61RtwxZsKiHi1pY84+WBq19WzeukaCku/2lghnUrz3G//5a00tAzBcCiz4a2xoLBXAXVr6729JkUoLItTWOY93047NNc3M+TAPb9x/H7ShK2U2kI4EuLYc4/i2HOP6vDX7rVbKWsr11NQGseyDDVf1nlV/oDBB+zBWT85hRuPvZ2CzQr/O2kHO+0Qyfda4UBrNwokm1M01bUQK8pnzGmH8OKjrxKOhgjnR0i1pEi1pBh/9hh26d+rw99PV9I+bKVUl5p4/tHYSRs7bRMrjlE+rB/99tyVXruV8qPfnOMtumlnqbed8gYaI3lhbzd329v811tBKbQ0tHDseeM5++bT+OEtpxMvjtFU00Re3OvKmXRr7u90ry1spVSXGnv6oVR9+rk38Biw8FYNGibfdgbDRg3GcRzK+pZQV13fZgdyK+i1L6PxKPGSGGtXrSeVSIMIIsJRPxjN9y4/DsuymDD5SL4zaRypRIpQJNRuTe9cpAtnlFK+WLtqHYveWoIVDLD/2L3b7H349osfcM+PHgKBSDyCnbJJtSTx6pUYCsu8Hd6TzSkaNjSw18FDuPVv12V1vZIdpSsdlVI5Z+Gbi/n7vf9i2YcrKSyN851J4zhg/HDuvexhViyozNSuHjxyD674/YWU7FLkd8gdQhO2UqrbEBGWzV/B+qoN7DKg13Z3x8k1ujRdKdVtGGMYPGIPBo/Yw+9Qulz36IlXSqkeQBO2UkrlCO0SUR1KJIE0Pw2Jf3qbAYcOwOSfhQnt5XdoSuU8bWGrDiNie3tLNj0Ebo1X6CE5G6m9DEnN9zs8pXKeJmzVcVJvQeoDMCXefpImAlYpiIM03e93dErlvKxI2JdffjkDBw7EGMMHH3zgdzhqJ0lyLuCC2XzX7AJIf4y4tX6EpVS3kRUJ+9RTT2XOnDnsvvvufoeivgmztSGRjZv36ZCJ2nkiCcRt9DsMX2XFb9CYMWP8DkF1ABM5Aml53tuezAS+OiH1ED4QY8W3/mSltkKcz5HG+yE1B8RFQnthYhdhwiP8Dq3LZUULe0dVVFRQXl6eeTQ29uxP26wTGgnR8V6CduvAbQR3A1hxTPwSv6NTOUjcGqTmMki+BsTAFEH6U6TuGiT9kd/hdbmcSthXXXUVVVVVmUc8ri22bGKMhSm4EVP4YwjvB8H+kP99TMkfMMHcLhyv/CEtL4BbDaYUTMj75mYVtw5k/9Hv8LpcVnSJqO7DmABEj8ZEj/Y7FNUdpN8GglvuAGxikJ6PiHSrOiLbk1MtbNX9iNuAOKsRSfkdispGpgBob4d0B0xej0rWkCUJe8qUKZkKVRMmTGDw4MF+h6Q6mbi1uPW3IetPQqp/gFSfgtv0V0TcbT+vtVi96nriNiL28i6dqWGiE1pvvsnmuSIgzRCd2GVxZIus6BJ54IEH/A5BdSERB6m7HtKfgCkEgiAJaHoAwcbEfrjlc5wvkaZHITkLcJDwoZjYuZhgz6vY1tVEWpDG+yDxr9YZQEEkehwmfjHG5HXuzcOHQfS41nu7ZNqYwWGY/P/u3HtnoaxI2Mo/4lRDchbiVmOCAyEypvN/CVPvQPrT1oGkjV9p8wADzdOQvFMx1ldbQ4m7Aam5BNx1YOJAyFvynnoXSn6PCer8/c4k9Xd6szRMAVhhkBS0/B1xazFFt3XqvY2xoOAaiI5HErOABCZ8cOu/00in3jsbacLuwSQ5F6m/BUh581uxIPAwFN2FCfbvvBvbn+KtiNx8ICkKbi04VWAN/SrOlr+Du75tgjel4G5Amv+CKbyp82Lt4cReBcnXwRR/NbfehIFiSL6O2KswwQGdGoMxljePP3xgp94nF2RFH7bqeuI2IPW3en8xJWCVeb+Uzjqk4Y7O7Se2CtourMkE5QDind9U8k0g3E6Cz4fU3M6KUgHYS70VrJv//zIB77i9zJ+4eqgek7DFXopbfxvu+hNwq7+P2/QY4jb7HZZ/Um8AKW961EbGtC5M+AScVZ137/ARQMAbONpIBKQOQsMxgb5trzdRoL3BSBdo+7VYRBB7KZJ802sdqm/GKvT6jjf/ABfxjluF/sTVQ/WILhFJf4LUXgGS9FplUgNNf0BS86C4AmPCfofY9dx675du81lRxgIsb7ViJzGBMqTgJmi43SvDSutgUmBXTOENW14fneCtapNNCkuJgLRA3qmZ68T50uviSX/itQDFQcIHYwp/osvid1ZoBAR6gbPe+wa2kdRBoDeE9vcrsh6pZyTspge8ZG2VbnI0D9ILITnbW07d0wRbp07KZtX1JIWXPDt3IM+KHomEvuUNJEk1Jjhk6wOe0e94s0NS74EYvE8ZF4KDMflneGGLi9TdAPby1v5WC3Ag9RbScAem6I5OfT/dlTFBKLwNqbsOpNabXmeCXrmBwlu986rLdPuftkhLa43mzb66GQsEJPUGpicm7NAICH3L+9Ci0Fv2KwmvmyL/+5gu+KprAn0wsTO3f50JQ9Gd3iBXcgZIGhMZB5HxX80mSb8P9orWWtwbByYDQBEk5yJ2FSZY3llvpVszob2g9K+QfA1xPscE+kFkrH5r8UG3T9hgtf4CtzeIJvSIH0E7jLGg6BdI492QfBVcb+UYsR9i8if5Hd4WjAlve8m787n3IbzFwGQACID7OaAJe2cZKw55x2/Rg6a6VrfPVsZEkPAh3kwDs0mXiLiAwUTG+hab34xViCm8GXEv96rrBXp3/hzszmL1AloHxzZN2uJ6X+Ot3r6FplRH6fYJG8DELkLSC7xSn4TxahM4EBkN4UN8js5/xioCq8jvML6Z8EFg7QrulyDFXtIW1+t3De+nKyJVt9AzEnZwAJQ8hLQ8B6n/gIlh8iZ6faDtzQdWOceYkNfFU3cTuGu8wUlxITQMU/BTv8NTqkMYyeFKOhsLRim1kYjtDUA66yCwmzeve/M9Jtt7ntsA6Y+9wdfQvj1zqqfKCtvKaz2iha1yk0gK3AawinZ4+pgxQQiP+hr3EKT5r9D0KNCSmasv8Wuw8k/YyciV6hw7vdLxxz/+cUfGoVSGSAtu473I+v9Cqr+HVJ+K2/zEdkuv7pTkDGh8wKtV4tZ60xrddVB/A26jVpFU2WWnE/Zjjz3WkXEoBbS2eOtugean8HYaKfPmhzf+Hml+tOPv1/yX1iXyrQuGCAIh72TTg4i9tMPvqdTO2ub3zJEjR7Z7XERYu3ZtpwSkejh7EaTmtS6A2dieyAcC0Px/SN5pHbuox64EWvCS9aazjC2QNJKYgYnrhhoqO2wzYX/22WdMmzaN/Pz8NsdFhDPOOKNTA1M9lP0J3mKnzb78mYjX0raXQfiAjrufVda6qGbzXwXxlmC71R13L6W+oW0m7AMOOICioiIOO+ywLc6FwzqKrjqBiW1ZkApa51S7basLdoS870PDQrwCVBuneIr3MPmY0L4dez+VISLgfuEtbAqU79Bsnp5umwn7j3/8I4WF7X/9/PTTTzslINXDhQ8FQl6/stn0m109BPt/VbSqg5j8k5Dkq5B6GS9ptxaXMgVg9YFID6wz0wUkvRBp+LVXbxsg0Afil2Eih/sbWJbb5kfa7rvvTklJCRMnTmTDhg2Z48uWLeOII47o9OBUz+Mtl/8x4HgrU90a72HimIKfdHgrzBgLU3I3FNwEVr/WRF0GkcMxxb/RAkedQOxKpPYasD9rraxYAk41Un8zkvrA7/Cy2g5Nbj3qqKMYNWoUjz/+OJWVlVxzzTVUVFR0dmyqhzKRMVD6JyTxEjhrIDgIEz0GYxV3zv2MwcT+G8n/PjhfeF0hgV6dcq/OIM5q70MtUN5pP6OOJInnvFrmm5Y7NgXg1iDNj2HCI/wKLevtUMK+5pprGDVqFEceeSRlZWXMnj2bPffcs7NjUz2YCfTDxCZ37T1NCDp5f8KOJM6XSMOdXvlgAmAMEj0BE78ou1dqpj7Eq+mzGZMP6UVdHk4u2aHvlytWrODaa69l0qRJDBw4kNtvv51EItHZsaluQpJv4dZe623NVncjknrP75Bynkgaqb22tdb7xuJdUWh5Fsn2BT+BXkC6nRM25MA3BD/tUMI+4ogjuPrqq3nggQd4/fXXKSkp4dvf/nZnx6a6Abf5CaTuRki945VwTb2F1F6D2/KC36HlttQ8b3d5U9J2N3MTh8TziNt5W7x9Uyb6Xe8PsknSFsebtpl3ki8x5Yod6hKZOXMmQ4cOBSAQCDB16lSmT5/eqYGp3CfuBmh6yPuqazZulpsHtEDjb5HIuK92jFFfj7MScNvZsCHs9Q87q7N3g9zwYZB/KjQ/3VqXXoAARI7AaMLeph1K2BuT9aaOP/74Dg9GdTOp9/HmM7fd2RyT57W20x9B5GBfQst5bVaCbkKc1t3MS7c8lyWMMZj4JUh0grd9m9iY8EgI7YfZ/ANItaHV+lQn2kblXrOd82rbImOg8d7WaoYF3jGR1g0bDsYEdvU1vB1hgoO9jZT9DiSHZM3SoiVLlnDYYYcxdOhQRo0axcKFC/0OSX1TodYl5JJse1xagBCEhnd5SN2FseKYop+DFQap86b1Sa03BbLger/DU50ka1rYU6ZM4cILL2Ty5Mk8/fTTTJ48mbffftvvsNQ3YAJlSP650PQHL0lvrAdigNj/YKwOXmbew5jwSCh9AlJvtM7D3gPCB+kuSt1YVuw4s3btWgYPHsyGDRsIBoOICH379mXOnDkMHrz1pci640z2ExFIvYE0P+UNhAX3wOSfjvkamwwo1ZNk/Y4zlZWV9O3bl2DQC8cYw4ABA1i1atU2E7bKfsYYiIzGREb7HYpSOS9r+rB3REVFBeXl5ZlHY2Oj3yEppVSX0S4RpZTKItvKa1nRwt5ll10YOXIkjz/+OADPPPMM5eXl2h2iuoy4jbiNf8BdfzLuuom4dT9G0ov9DkupNrKihQ2wePFiJk+eTHV1NYWFhTz66KMMH77taV/awlYdQSSF1F7uFR4yeUAApAlMGFP8a0xoH79DVD1I1g86AgwbNoy5c+f6HYbqiZKvQnoxmNKvlnqbiFfus/FBr162UlkgaxK22nEiKUjP91qBwb0wgT5+h5TTJNnaUNiiLkcBpD9EpAVj8ro+MKU2owk7x0hqPlJ/K0gN3ld3B4lOxBRc6dVzVl+fCdH+MnlprdehC1FUdsiKQUe1Y8RZj9TdAG49UAym0GsFJv6JND/md3g5y0SOBIxXOGlT0gDhw7N7MwDVo2jCziGSeNlb2m0VbtLXGvR2Em9+1usqUV9f+GCIjAOpB7fWK6jkbgCrFBOf0uG3E0kj6UXeQ9or5K9U+7RLJJc4K7dyIgJS7SWaQFmXhtQdGGNB4U8gORpJ/Auk0at4Fz0B08E/T0nOQRoqvNof4O2wUvA/3j6WSm2HJuxcEtjafoPJ1t2+C7o0nO7EmABEx2Oi4zvtHpJeiNT/FCTg7RYO4DZ5YxLFd2NC+3bavVX3oF0iOcREj2mdbtbg1T4GEBukGfJO1r7WLCfNT3j95Fbc69IyxvuzON45pbZDE3YOMYHemKI7wIp5NZClzhsYi07AxP7b7/DU9tiLWxfmbMbkgf1J18ejco52ieQYEx4JZU9C6r3WedjDMMFyv8NSO8LaBZz1WyZtSYGV/TvEKP9pws5BxoQhcojfYaivyeSdjKQXeLuFb5wzL2nAweSd7GtsKjdowlaqq0TGQf4n0Pxk23U6+adD5Ei/olI5RBO2Ul3E2y38YiR6PKTf8ZJ2+CBMcGuzf5RqSxO2Ul3MBAeAJmm1E3SWiFJK5QhN2EoplSO0S0QptVViLwd7ubeEPrS/tyJU+UYTtlJqC+I2Iw0/h+RcMAEQFwK7QNHPMUHdus8v2iWilNqCNN4NyTdaS/gWgikCdy1Sey0iLX6H12NpwlZKtSFuDSRntCbq1i4QY7yCVW4dJF/3Nb6eTBO2UqotZx3ebjvt7WDkgPNlV0ekWmnCVkq1FdgFbwee9jZXCIDuIeobTdhKqTaMVQyRo70deDZumyYCUgtWEUSO8DO8Hk1niSiltmDiVyDS6A08EvQSd6APpuhnuoO8jzRhK6W2YKx8TNHtiL2idR52kc7DzgKasJVSW2WCAyE40O8wVCvtw1ZKqRyhCVsppXKEJmyllMoRvifs6dOnc+CBBxKJRLjyyiv9DkcppbKW74OOQ4YM4ZFHHuGpp56isbHR73CUUipr+d7CHjp0KPvvvz/BoO+fHUopldV8T9hfR0VFBeXl5ZmHtsiVUj1JpyfsQw89lF69erX7qKys/FqvddVVV1FVVZV5xOPxTopaKaWyT6f3Q8ydO7ezb6GUUj1CTnWJKKVUT+Z7wp45cybl5eVUVFTw8MMPU15ezvPPP+93WEoplXV8n5oxfvx4qqqq/A5DKaWynu8tbKWUUjtGE7ZSSuUITdhKKZUjNGErpVSO0IStlFI5QhO2UkrlCE3YSimVIzRhK6VUjtCErZRSOUITtlJK5Qjfl6arziFuPdL8BCRfArEhfBgm/0xMsNzv0JRSO0kTdjckbiNSewXYn4HJAyxITEdSr0Px7zDBAX6HqJTaCdol0g2IOIi9FLGXI+IiiZfAXg6mFEw+mChYpeA2IM1/9jtcpdRO0hZ2jpPkbKTxHnDWAwKBfkAACIIxbS82MUi+4UOUSqmOoAk7h0lqPlJ/C0gATLF30FkL7nqwCtt7BvqlSqncpb+9OUya/wLighX3WtPGeInaREAavXOZiwWkCaLj/QtYKfWNaAs7l9mLWwcVN2MKQRpAakGCeJ/LSQj0weSf3cVBKqU6iibsXGaVgl3lDSq24UD4UEx0rDcAKUmIjMbknYixSn0JVSn1zWnCzmEm72Sk4S5vnrVp/V8paUAw+SdjIkdg8k7yM0SlVAfShJ3LohMhvRAS//bGEwEwkH8ahEf7GZlSqhNows5hxgSg4HrIOxnS7wAWhA/GBPfwOzSlVCfQhJ3jjDEQGuo9lFLdmibsbk7cOrCXeSseg0MxRmdyKpWrNGF3UyIu0vQwtDwJuN6c7EA5FP4YE9qri2OxIfU2OCvBKoPw4Rgrv0tjUKo70ITdTUnLU9D8FzBxMGFAwKlC6q6F0j9jrJKuicP5Eqm7DpxVrQOjBqwYFN2BCQ3vkhiU6i70+3E3JGJD8zRvfrYJeweNAasE3CYkMaPrYmm4A+yVQLF3f6sY3Bak7iZEWrosDqW6A03Y3ZFbB+4GoJ1VkAjYS7skDLErITUfTFHbQlRWIbhNkHyzS+JQqrvQhN0dWQXeICOpdk4KBPp2TRzuBjAB79FeHG5118ShVDfhe8K+55572HfffRk+fDj77bcfjz/+uN8h5TxjwpD33XYKQDWDCWGi3+maQIL9W++bbntcWlf56Hxxpb4W3wcd99lnH9544w2KioqorKzkgAMO4NBDD2XQoEF+h5bTTOw8xP4cUm+CWK1dEiFM4f/DBPp1TQxWKRI5DhL/BArAhEAckDoI7gmhkV0Sh1Ldhe8Je/z4r8p99u/fnz59+lBZWakJ+xsyJg+Kbvcq+tmfeF0k4cMwVrxr4yi4HDHeFmUIXos/fCCm4EZvpaZSaof5nrA3NWPGDGpqahg1alS75ysqKqioqMj8vbGxsatCy0neKsi9vIdvMYQxBVchsXPAqQKrrMta+Ep1N0ZEZPuX7bxDDz2UJUuWtHvu/fffp39/r5/zo48+YuLEiUybNo3Ro3escFF5eTlVVVUdFqtSSvltW3mt01vYc+fO3e41H3/8Md/97nd55JFHdjhZK6VUT+P7LJFFixYxceJEHnzwQY455hi/w1FKqazle8K+/PLLqaur4/rrr2fEiBGMGDGCF1980e+wlFIq63R6H3ZnikQi9O7de6vnGxsbice7dlZENtD33bP0xPfdnd/zunXrSCaT7Z7L6YS9PT11UFLfd8/SE993T3zPkAVdIkoppXaMJmyllMoR3TphX3XVVX6H4At93z1LT3zfPfE9Qzfvw1ZKqe6kW7ewlVKqO9GErZRSOaLbJ+yeWm97+vTpHHjggUQiEa688kq/w+lUS5Ys4bDDDmPo0KGMGjWKhQsX+h1Sp7v88ssZOHAgxhg++OADv8PpMolEgpNOOomhQ4ey//77c8wxx7B0adfsoJQNun3C3lhv+6OPPmL69OlceeWVLFu2zO+wOt2QIUN45JFHuPbaa/0OpdNNmTKFCy+8kE8//ZTrr7+eyZMn+x1Spzv11FOZM2cOu+++u9+hdLkLL7yQxYsXM3/+fE488UTOP/98v0PqMt0+YY8fP56ioiKgbb3t7m5jCyQYzKoKuh1u7dq1vPPOO5x99tkAnHLKKVRWVnb7VteYMWMoLy/3O4wuF41GmThxolc6GDjkkENYsWKFv0F1oW6fsDe1vXrbKvdUVlbSt2/fzAeTMYYBAwawatUqnyNTXeHuu+/mxBNP9DuMLpPzza+vU2/7nHPO4YknniAWi3VliJ1iR9+3Ut3VHXfcwdKlS5k5c6bfoXSZnE/YPbXe9o68756gf//+fPHFF9i2TTAYRERYtWoVAwYM8Ds01YmmTp3Ks88+y4wZM8jPz/c7nC7T7btEtN5297bLLrswcuTIzOyfZ555hvLycgYPHuxzZKqzVFRUMG3aNF5++WWKi4v9DqdLdfuVjscccwzvvPNOm9H0X/7yl0yYMMHHqDrfzJkzmTRpEvX19YgIRUVF3HfffZxwwgl+h9bhFi9ezOTJk6murqawsJBHH32U4cOH+x1Wp5oyZQrTp09nzZo1lJWVUVBQ0O0HWgGqqqro378/e+65JwUFBYBXZnnevHk+R9Y1un3CVkqp7qLbd4kopVR3oQlbKaVyhCZspZTKEZqwlVIqR2jCVkqpHKEJWymlcoQmbKV2wMMPP8yQIUMYNGgQF1xwAel02u+QVA+kCVup7Vi+fDk333wzs2fPZunSpXz55Zc8+OCDfoeleiBN2Eq1Wrx4MeXl5Xz22WeAV6/i2GOP5cknn+SEE06gT58+GGO46KKLmDZtms/Rqp5IE7ZSrYYNG8b//u//cvrpp/Pqq6/yu9/9jscee4yqqqo2pQ0GDhyo5VuVL3K+Wp9SHenMM89k1qxZTJgwgZkzZ9K7d2+/Q1IqQ1vYSm3Ctm0WLFhAaWkpq1evBmDAgAGsXLkyc82KFSu0fKvyhSZspTZxww03MGzYMGbPns0111zD0qVLOeWUU3j++edZs2YNIsL999/P97//fb9DVT2Qdoko1eqf//wn//73v/nPf/5Dfn4+FRUVnH766bz55pvceuutHH744QCMGzeOKVOm+Byt6om0vKpSSuUI7RJRSqkcoQlbKaVyhCZspZTKEZqwlVIqR2jCVkqpHKEJWymlcoQmbKWUyhGasJVSKkf8f5mdfMNkeKeiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 400x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(5, 3), dpi=80)\n",
    "plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)\n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453e735-4c51-4ab3-93e0-a1b83fd15a42",
   "metadata": {},
   "source": [
    "### Step 1: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5b38f5-3d40-43b2-a465-1810e5a0b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 00:21:14.397715: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 00:21:14.398512: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                75        \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.9446 - val_loss: 3.6699\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 3.1922 - val_loss: 3.1777\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 2.8389 - val_loss: 2.8910\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 2.5223 - val_loss: 2.7019\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 2.4262 - val_loss: 2.5498\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 2.2960 - val_loss: 2.4134\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 2.1888 - val_loss: 2.3343\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 2.1099 - val_loss: 2.2643\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 2.0617 - val_loss: 2.2098\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 2.0137 - val_loss: 2.1609\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.9724 - val_loss: 2.1156\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.9313 - val_loss: 2.0722\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.8834 - val_loss: 2.0314\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.8495 - val_loss: 1.9915\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.8104 - val_loss: 1.9517\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.7740 - val_loss: 1.9146\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.7507 - val_loss: 1.8780\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.7084 - val_loss: 1.8439\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.6888 - val_loss: 1.8097\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.6503 - val_loss: 1.7768\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.6230 - val_loss: 1.7490\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.5859 - val_loss: 1.7206\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.5685 - val_loss: 1.6925\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.5409 - val_loss: 1.6666\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.5171 - val_loss: 1.6366\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.5079 - val_loss: 1.6168\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4761 - val_loss: 1.5972\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.4651 - val_loss: 1.5756\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.4427 - val_loss: 1.5584\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.4339 - val_loss: 1.5429\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.4244 - val_loss: 1.5267\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.4138 - val_loss: 1.5129\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 1.3978 - val_loss: 1.5020\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.3745 - val_loss: 1.4899\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.3775 - val_loss: 1.4765\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.3669 - val_loss: 1.4648\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3460 - val_loss: 1.4534\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3383 - val_loss: 1.4445\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.3380 - val_loss: 1.4354\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.3213 - val_loss: 1.4269\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3241 - val_loss: 1.4179\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.3136 - val_loss: 1.4097\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3055 - val_loss: 1.4012\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3023 - val_loss: 1.3928\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2840 - val_loss: 1.3855\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.2765 - val_loss: 1.3792\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2703 - val_loss: 1.3722\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2666 - val_loss: 1.3668\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2565 - val_loss: 1.3626\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2601 - val_loss: 1.3570\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2444 - val_loss: 1.3499\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.2447 - val_loss: 1.3439\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2350 - val_loss: 1.3361\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2442 - val_loss: 1.3317\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2319 - val_loss: 1.3260\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2323 - val_loss: 1.3207\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2318 - val_loss: 1.3169\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2177 - val_loss: 1.3142\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2121 - val_loss: 1.3100\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2027 - val_loss: 1.3073\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.2220 - val_loss: 1.3037\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1950 - val_loss: 1.2999\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2123 - val_loss: 1.2962\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1883 - val_loss: 1.2938\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1770 - val_loss: 1.2884\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.2012 - val_loss: 1.2863\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1846 - val_loss: 1.2834\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1754 - val_loss: 1.2799\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1783 - val_loss: 1.2751\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1768 - val_loss: 1.2719\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.1707 - val_loss: 1.2687\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1673 - val_loss: 1.2649\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1639 - val_loss: 1.2622\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1625 - val_loss: 1.2594\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1646 - val_loss: 1.2569\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1645 - val_loss: 1.2544\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1647 - val_loss: 1.2542\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.1632 - val_loss: 1.2511\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.1543 - val_loss: 1.2465\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.1626 - val_loss: 1.2450\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1467 - val_loss: 1.2419\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.1477 - val_loss: 1.2396\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1454 - val_loss: 1.2379\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1464 - val_loss: 1.2375\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1492 - val_loss: 1.2347\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.1453 - val_loss: 1.2303\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.1341 - val_loss: 1.2292\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1439 - val_loss: 1.2273\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1361 - val_loss: 1.2264\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1297 - val_loss: 1.2240\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1298 - val_loss: 1.2207\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.1279 - val_loss: 1.2170\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1316 - val_loss: 1.2165\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.1279 - val_loss: 1.2117\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1256 - val_loss: 1.2107\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.1307 - val_loss: 1.2134\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1252 - val_loss: 1.2115\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.1208 - val_loss: 1.2081\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1158 - val_loss: 1.2056\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1210 - val_loss: 1.2040\n",
      "The threshold for the defined contamination rate: 4.132258340528481\n",
      "The training data: {0: 475, 1: 25}\n",
      "The training data: {0: 475, 1: 25}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr.fit(X_train)\n",
    "\n",
    "# Training data\n",
    "y_train_scores = atcdr.decision_function(X_train)\n",
    "y_train_pred = atcdr.predict(X_train)\n",
    "\n",
    "# Test data\n",
    "y_test_scores = atcdr.decision_function(X_test)\n",
    "y_test_pred = atcdr.predict(X_test) # outlier labels (0 or 1)\n",
    "\n",
    "# Threshold for the defined comtanimation rate\n",
    "print(\"The threshold for the defined contamination rate:\" , atcdr.threshold_)\n",
    "\n",
    "def count_stat(vector):\n",
    "    # Because it is '0' and '1', we can run a count statistic. \n",
    "    unique, counts = np.unique(vector, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "print(\"The training data:\", count_stat(y_train_pred))\n",
    "print(\"The training data:\", count_stat(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a150d79-9337-4890-ae82-bb312d166a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'contamination': 0.05,\n",
       " 'dropout_rate': 0.2,\n",
       " 'epochs': 100,\n",
       " 'hidden_activation': 'relu',\n",
       " 'hidden_neurons': [2, 2],\n",
       " 'l2_regularizer': 0.1,\n",
       " 'loss': <function tensorflow.python.keras.losses.mean_squared_error(y_true, y_pred)>,\n",
       " 'optimizer': 'adam',\n",
       " 'output_activation': 'sigmoid',\n",
       " 'preprocessing': True,\n",
       " 'random_state': None,\n",
       " 'validation_size': 0.1,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atcdr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1943f6f-327d-497b-9a4a-5ef69864ddc1",
   "metadata": {},
   "source": [
    "### Step 2: Determine the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9015e76e-d382-4bfb-867f-96eccef1d5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_scores, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Outlier score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33284af5-5a22-465a-ad83-78bb81e3cd9e",
   "metadata": {},
   "source": [
    "### Step 3: Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b6f4a1-7215-4874-a32a-450a70ff1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>20.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5    6  ...  \\\n",
       "0   Normal    475     95.0  2.01  2.01  2.02  2.00  2.00  2.00  2.0  ...   \n",
       "1  Outlier     25      5.0 -0.02  0.21 -0.05  0.13  0.01 -0.32  0.2  ...   \n",
       "\n",
       "     16    17    18    19    20    21   22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.0  2.01  1.99           2.11  \n",
       "1 -0.07 -0.30  0.11  0.18  0.14 -0.28  0.2  0.09 -0.10          20.26  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = atcdr.threshold_ # Or other value from the above histogram\n",
    "\n",
    "def descriptive_stat_threshold(df,pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score']< threshold, 'Normal', 'Outlier')\n",
    "\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index() # The avg.\n",
    "    stat = cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together\n",
    "    return (stat)\n",
    "\n",
    "descriptive_stat_threshold(X_train,y_train_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a42470-3753-4067-a35d-9832310d5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99           2.10  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36          20.92  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf7362d-5fd3-4151-a61e-b148a7607c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(actual,score, threshold):\n",
    "    Actual_pred = pd.DataFrame({'Actual': actual, 'Pred': score})\n",
    "    Actual_pred['Pred'] = np.where(Actual_pred['Pred']<=threshold,0,1)\n",
    "    cm = pd.crosstab(Actual_pred['Actual'],Actual_pred['Pred'])\n",
    "    return (cm)\n",
    "confusion_matrix(y_train,y_train_scores,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de5405d9-690b-43ad-a4a1-6c32156ac9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_scores,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91fcbc-aa09-4c08-b0c7-d77310ccc904",
   "metadata": {},
   "source": [
    "### Step 4: Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d2bb66-b6ef-44fc-9c9f-0dbfb4b484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                75        \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 3.6846 - val_loss: 2.9134\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 3.1057 - val_loss: 2.6097\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 2.8505 - val_loss: 2.3851\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 2.6101 - val_loss: 2.2408\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 2.4406 - val_loss: 2.1299\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 2.3324 - val_loss: 2.0440\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 2.2348 - val_loss: 1.9768\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 2.1677 - val_loss: 1.9210\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 2.1129 - val_loss: 1.8735\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 2.0624 - val_loss: 1.8320\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 2.0162 - val_loss: 1.7961\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.9801 - val_loss: 1.7641\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 1.9467 - val_loss: 1.7353\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.9157 - val_loss: 1.7091\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.8878 - val_loss: 1.6843\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.8645 - val_loss: 1.6622\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.8420 - val_loss: 1.6410\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.8181 - val_loss: 1.6222\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.7998 - val_loss: 1.6042\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 1.7800 - val_loss: 1.5878\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.7611 - val_loss: 1.5718\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.7446 - val_loss: 1.5558\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.7284 - val_loss: 1.5402\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.7135 - val_loss: 1.5255\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.6978 - val_loss: 1.5112\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.6849 - val_loss: 1.4975\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.6707 - val_loss: 1.4845\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.6576 - val_loss: 1.4720\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.6452 - val_loss: 1.4597\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.6327 - val_loss: 1.4479\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.6209 - val_loss: 1.4367\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.6097 - val_loss: 1.4257\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.5983 - val_loss: 1.4146\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.5873 - val_loss: 1.4039\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.5767 - val_loss: 1.3938\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.5663 - val_loss: 1.3841\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.5562 - val_loss: 1.3743\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.5466 - val_loss: 1.3649\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.5371 - val_loss: 1.3560\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.5279 - val_loss: 1.3466\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.5187 - val_loss: 1.3378\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.5101 - val_loss: 1.3295\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.5015 - val_loss: 1.3211\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.4931 - val_loss: 1.3130\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.4850 - val_loss: 1.3051\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.4771 - val_loss: 1.2975\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.4691 - val_loss: 1.2897\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.4612 - val_loss: 1.2824\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.4538 - val_loss: 1.2752\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.4465 - val_loss: 1.2682\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.4395 - val_loss: 1.2614\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.4324 - val_loss: 1.2548\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.4257 - val_loss: 1.2481\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.4190 - val_loss: 1.2417\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.4125 - val_loss: 1.2354\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4060 - val_loss: 1.2294\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3999 - val_loss: 1.2232\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3936 - val_loss: 1.2174\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.3877 - val_loss: 1.2117\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.3818 - val_loss: 1.2061\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.3760 - val_loss: 1.2007\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3704 - val_loss: 1.1952\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.3649 - val_loss: 1.1898\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.3594 - val_loss: 1.1846\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3540 - val_loss: 1.1796\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.3488 - val_loss: 1.1745\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3437 - val_loss: 1.1697\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.3387 - val_loss: 1.1649\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3337 - val_loss: 1.1603\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.3289 - val_loss: 1.1557\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3242 - val_loss: 1.1512\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.3196 - val_loss: 1.1467\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3150 - val_loss: 1.1423\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.3105 - val_loss: 1.1382\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3062 - val_loss: 1.1341\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3019 - val_loss: 1.1300\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2977 - val_loss: 1.1259\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2935 - val_loss: 1.1220\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2895 - val_loss: 1.1181\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2854 - val_loss: 1.1143\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.2812 - val_loss: 1.1103\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2772 - val_loss: 1.1065\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2734 - val_loss: 1.1030\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2697 - val_loss: 1.0995\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2661 - val_loss: 1.0960\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2625 - val_loss: 1.0927\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2589 - val_loss: 1.0894\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2555 - val_loss: 1.0863\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2522 - val_loss: 1.0831\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2489 - val_loss: 1.0800\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2456 - val_loss: 1.0769\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2425 - val_loss: 1.0738\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2392 - val_loss: 1.0709\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2362 - val_loss: 1.0680\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2331 - val_loss: 1.0652\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2301 - val_loss: 1.0624\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.2272 - val_loss: 1.0596\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.2243 - val_loss: 1.0569\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.2215 - val_loss: 1.0542\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2186 - val_loss: 1.0516\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 25)                275       \n",
      "=================================================================\n",
      "Total params: 1,887\n",
      "Trainable params: 1,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.3301 - val_loss: 4.3864\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 2.8205 - val_loss: 3.8617\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 2.5404 - val_loss: 3.5281\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 2.3440 - val_loss: 3.3095\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 2.2261 - val_loss: 3.1583\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 2.1382 - val_loss: 3.0317\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 2.0655 - val_loss: 2.9347\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 2.0080 - val_loss: 2.8448\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.9533 - val_loss: 2.7789\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.9084 - val_loss: 2.7268\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.8730 - val_loss: 2.6768\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.8357 - val_loss: 2.6305\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 145us/sample - loss: 1.8000 - val_loss: 2.5899\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.7678 - val_loss: 2.5515\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.7359 - val_loss: 2.5138\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.7019 - val_loss: 2.4708\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 1.6722 - val_loss: 2.4339\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.6363 - val_loss: 2.4006\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.6087 - val_loss: 2.3632\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.5711 - val_loss: 2.3243\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.5439 - val_loss: 2.2871\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.5062 - val_loss: 2.2485\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 169us/sample - loss: 1.4925 - val_loss: 2.2145\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 173us/sample - loss: 1.4470 - val_loss: 2.1811\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 172us/sample - loss: 1.4212 - val_loss: 2.1506\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 163us/sample - loss: 1.4164 - val_loss: 2.1235\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.3965 - val_loss: 2.0985\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.3553 - val_loss: 2.0763\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.3488 - val_loss: 2.0547\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.3282 - val_loss: 2.0363\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.3230 - val_loss: 2.0205\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 1.3064 - val_loss: 2.0038\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.2944 - val_loss: 1.9891\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.2771 - val_loss: 1.9746\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.2715 - val_loss: 1.9632\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.2744 - val_loss: 1.9517\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.2467 - val_loss: 1.9360\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.2520 - val_loss: 1.9253\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.2305 - val_loss: 1.9179\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.2304 - val_loss: 1.9112\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.2237 - val_loss: 1.9046\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.2166 - val_loss: 1.8966\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.2149 - val_loss: 1.8879\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.2029 - val_loss: 1.8806\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.1901 - val_loss: 1.8744\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.1892 - val_loss: 1.8678\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1828 - val_loss: 1.8606\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.1786 - val_loss: 1.8533\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.1754 - val_loss: 1.8481\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1760 - val_loss: 1.8445\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1729 - val_loss: 1.8408\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1667 - val_loss: 1.8365\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.1544 - val_loss: 1.8310\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1562 - val_loss: 1.8248\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.1465 - val_loss: 1.8205\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.1539 - val_loss: 1.8183\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.1452 - val_loss: 1.8146\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.1314 - val_loss: 1.8121\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 140us/sample - loss: 1.1378 - val_loss: 1.8068\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.1269 - val_loss: 1.8002\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.1345 - val_loss: 1.7975\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.1278 - val_loss: 1.7941\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.1239 - val_loss: 1.7910\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.1140 - val_loss: 1.7881\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.1172 - val_loss: 1.7867\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.1161 - val_loss: 1.7836\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.1154 - val_loss: 1.7792\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.1128 - val_loss: 1.7765\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0985 - val_loss: 1.7746\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.1040 - val_loss: 1.7711\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0965 - val_loss: 1.7652\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0974 - val_loss: 1.7630\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0903 - val_loss: 1.7620\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.0968 - val_loss: 1.7593\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0909 - val_loss: 1.7562\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0946 - val_loss: 1.7522\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0894 - val_loss: 1.7470\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.0772 - val_loss: 1.7440\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0809 - val_loss: 1.7429\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.0692 - val_loss: 1.7410\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0758 - val_loss: 1.7412\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.0828 - val_loss: 1.7387\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0756 - val_loss: 1.7356\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0693 - val_loss: 1.7350\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.0742 - val_loss: 1.7341\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.0712 - val_loss: 1.7305\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.0676 - val_loss: 1.7293\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0650 - val_loss: 1.7249\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.0659 - val_loss: 1.7231\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0566 - val_loss: 1.7158\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0646 - val_loss: 1.7154\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0600 - val_loss: 1.7128\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.0573 - val_loss: 1.7128\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0532 - val_loss: 1.7129\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0588 - val_loss: 1.7260\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0540 - val_loss: 1.7190\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.0604 - val_loss: 1.7139\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0490 - val_loss: 1.7080\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0446 - val_loss: 1.7071\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.0556 - val_loss: 1.7051\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                400       \n",
      "=================================================================\n",
      "Total params: 2,467\n",
      "Trainable params: 2,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.9857 - val_loss: 2.3579\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 3.2060 - val_loss: 2.1323\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 2.8417 - val_loss: 1.9801\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 2.5746 - val_loss: 1.8800\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 2.4314 - val_loss: 1.7958\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 2.3030 - val_loss: 1.7059\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 2.1725 - val_loss: 1.6096\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 2.0572 - val_loss: 1.5281\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.9640 - val_loss: 1.4522\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.8811 - val_loss: 1.3853\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 166us/sample - loss: 1.7899 - val_loss: 1.3252\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.7193 - val_loss: 1.2741\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.6717 - val_loss: 1.2326\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.6178 - val_loss: 1.1991\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.5848 - val_loss: 1.1712\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.5431 - val_loss: 1.1448\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 164us/sample - loss: 1.5107 - val_loss: 1.1202\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.4958 - val_loss: 1.0979\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.4669 - val_loss: 1.0801\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.4459 - val_loss: 1.0638\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.4263 - val_loss: 1.0491\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.4096 - val_loss: 1.0359\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.3935 - val_loss: 1.0236\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.3753 - val_loss: 1.0100\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.3667 - val_loss: 0.9990\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.3576 - val_loss: 0.9897\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.3444 - val_loss: 0.9804\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.3293 - val_loss: 0.9718\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.3202 - val_loss: 0.9637\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.3107 - val_loss: 0.9565\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 166us/sample - loss: 1.3024 - val_loss: 0.9494\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2939 - val_loss: 0.9428\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2876 - val_loss: 0.9349\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2780 - val_loss: 0.9274\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.2810 - val_loss: 0.9205\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2629 - val_loss: 0.9147\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2573 - val_loss: 0.9086\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2527 - val_loss: 0.9033\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2425 - val_loss: 0.8981\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.2435 - val_loss: 0.8935\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2347 - val_loss: 0.8906\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 172us/sample - loss: 1.2243 - val_loss: 0.8840\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2251 - val_loss: 0.8794\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2191 - val_loss: 0.8749\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2163 - val_loss: 0.8707\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.2064 - val_loss: 0.8672\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.2058 - val_loss: 0.8634\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.2018 - val_loss: 0.8597\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1957 - val_loss: 0.8557\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2003 - val_loss: 0.8523\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1917 - val_loss: 0.8495\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1921 - val_loss: 0.8466\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1826 - val_loss: 0.8438\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1824 - val_loss: 0.8407\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.1802 - val_loss: 0.8380\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.1754 - val_loss: 0.8353\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1771 - val_loss: 0.8326\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1674 - val_loss: 0.8313\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1717 - val_loss: 0.8284\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1638 - val_loss: 0.8256\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1640 - val_loss: 0.8233\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1601 - val_loss: 0.8214\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1582 - val_loss: 0.8193\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1588 - val_loss: 0.8173\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1561 - val_loss: 0.8152\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1561 - val_loss: 0.8136\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1490 - val_loss: 0.8118\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1478 - val_loss: 0.8101\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1485 - val_loss: 0.8086\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1437 - val_loss: 0.8067\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1424 - val_loss: 0.8051\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1450 - val_loss: 0.8050\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1404 - val_loss: 0.8032\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 168us/sample - loss: 1.1414 - val_loss: 0.8007\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1361 - val_loss: 0.7991\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1341 - val_loss: 0.7971\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1372 - val_loss: 0.7955\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1360 - val_loss: 0.7945\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1339 - val_loss: 0.7935\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1362 - val_loss: 0.7927\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1281 - val_loss: 0.7909\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1265 - val_loss: 0.7897\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1251 - val_loss: 0.7889\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1244 - val_loss: 0.7874\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1306 - val_loss: 0.7862\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1228 - val_loss: 0.7849\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.1249 - val_loss: 0.7829\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1212 - val_loss: 0.7833\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1163 - val_loss: 0.7823\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1213 - val_loss: 0.7806\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1136 - val_loss: 0.7790\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1166 - val_loss: 0.7780\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 171us/sample - loss: 1.1164 - val_loss: 0.7774\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1166 - val_loss: 0.7762\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1120 - val_loss: 0.7750\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1130 - val_loss: 0.7742\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 164us/sample - loss: 1.1144 - val_loss: 0.7733\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 170us/sample - loss: 1.1141 - val_loss: 0.7724\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 180us/sample - loss: 1.1075 - val_loss: 0.7718\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 1.1079 - val_loss: 0.7713\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr1 = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr2 = AutoEncoder(contamination=0.05, hidden_neurons =[10, 2, 10])\n",
    "atcdr3 = AutoEncoder(contamination=0.05, hidden_neurons =[15, 10, 2, 10, 15] )\n",
    "\n",
    "# Standardize data\n",
    "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "# Just prepare data frames so we can store the model results. There are three models.\n",
    "train_scores = np.zeros([X_train.shape[0], 3])\n",
    "test_scores = np.zeros([X_test.shape[0], 3])\n",
    "atcdr1.fit(X_train_norm)\n",
    "atcdr2.fit(X_train_norm)\n",
    "atcdr3.fit(X_train_norm)\n",
    "    \n",
    "# Store the results in each column:\n",
    "train_scores[:, 0] = atcdr1.decision_function(X_train_norm) \n",
    "train_scores[:, 1] = atcdr2.decision_function(X_train_norm) \n",
    "train_scores[:, 2] = atcdr3.decision_function(X_train_norm)\n",
    "test_scores[:, 0] = atcdr1.decision_function(X_test_norm) \n",
    "test_scores[:, 1] = atcdr2.decision_function(X_test_norm) \n",
    "test_scores[:, 2] = atcdr3.decision_function(X_test_norm)\n",
    "\n",
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec9923e-c4df-432e-96dd-e92e5b66497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "# Combination by average\n",
    "y_train_by_average = average(train_scores_norm)\n",
    "y_test_by_average = average(test_scores_norm)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_by_average, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c2b2e2-ebe3-490c-8008-c9984613a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>474</td>\n",
       "      <td>94.8</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>26</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    474     94.8  2.01  2.01  2.03  2.00  2.00  2.00  2.00  ...   \n",
       "1  Outlier     26      5.2  0.05  0.30  0.01  0.18  0.09 -0.24  0.28  ...   \n",
       "\n",
       "     16    17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.00  2.01  1.99          -0.23  \n",
       "1  0.00 -0.21  0.20  0.25  0.21 -0.18  0.27  0.18 -0.03           4.15  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a23053-4d1a-4d3e-992a-416439aa5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99          -0.23  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36           4.47  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19589ca4-8802-4014-9d14-3c378f31c39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     474   1\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f86490-1377-4b2c-8fad-21c622a33e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196290e-37dd-4328-a264-25b0d1b75436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b1996-c86d-4192-b5f5-57a1235c2fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fef3d4-4594-485f-86d8-e8e6538d7f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
